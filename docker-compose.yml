services:
  # Kafka using KRaft mode (no Zookeeper)
  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka-digital-twin
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_LOG_FLUSH_INTERVAL_MESSAGES: 9223372036854775807
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_HEAP_OPTS: "-Xms512m -Xmx1g"
      CLUSTER_ID: "vSwa19JxQAaHi1f4ee_08g"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092"
      # Topic configuration
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1
      # Performance settings
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 3000
      # Log retention settings
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_RETENTION_BYTES: 1073741824  # 1GB
      # Message size settings
      KAFKA_MAX_MESSAGE_BYTES: 1048588  # 1MB
      KAFKA_MESSAGE_MAX_BYTES: 1048588 # 1MB
      # Consumer settings
      KAFKA_OFFSETS_RETENTION_MINUTES: 10080  # 7 days
      # Performance and batch settings
      KAFKA_NUM_NETWORK_THREADS: 3
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_REPLICA_FETCH_MIN_BYTES: 1
      KAFKA_REPLICA_FETCH_WAIT_MAX_MS: 500
      KAFKA_FOLLOWER_REPLICATION_THROTTLED_REPLICAS: "*"
      KAFKA_LEADER_REPLICATION_THROTTLED_REPLICAS: "*"
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_ROLL_MS: 300000
      KAFKA_LOG_INDEX_SIZE_MAX_BYTES: 4096
      KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS: 60000
      # Producer settings
      KAFKA_PRODUCER_RETRY_BACKOFF_MS: 100
      # Consumer settings
      KAFKA_CONSUMER_MAX_POLL_INTERVAL_MS: 300000
      KAFKA_CONSUMER_SESSION_TIMEOUT_MS: 10000
      KAFKA_CONSUMER_HEARTBEAT_INTERVAL_MS: 3000
      # JMX settings for monitoring
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    mem_limit: 1.2g
    restart: unless-stopped

  # PostgreSQL for Digital Twin Service
  postgres-digital-twin:
    image: postgres:15
    container_name: postgres-digital-twin
    environment:
      POSTGRES_DB: digital_twin
      POSTGRES_USER: digital_twin_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password123}
    volumes:
      - postgres_digital_twin_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U digital_twin_user -d digital_twin"]
      interval: 10s
      timeout: 5s
      retries: 5
    mem_limit: 512m
    restart: unless-stopped

  # TimescaleDB for Alert/Analytics Service (optimized for time-series data)
  timescaledb-alert-analytics:
    image: timescale/timescaledb:latest-pg15
    container_name: timescaledb-alert-analytics
    environment:
      POSTGRES_DB: alert_analytics
      POSTGRES_USER: alert_analytics_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password123}
    volumes:
      - timescale_alert_analytics_data:/var/lib/postgresql/data
    ports:
      - "5434:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U alert_analytics_user -d alert_analytics"]
      interval: 10s
      timeout: 5s
      retries: 5
    mem_limit: 512m
    restart: unless-stopped
  # Device Simulator Service
  device-simulator:
    build:
      context: ./device-simulator
      dockerfile: Dockerfile
    container_name: device-simulator
    depends_on:
      kafka:
        condition: service_healthy
      kafka-setup:
        condition: service_completed_successfully
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    ports:
      - "8081:8081"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    mem_limit: 512m
    restart: unless-stopped

  # Digital Twin Service
  digital-twin:
    build:
      context: ./digital-twin
      dockerfile: Dockerfile
    container_name: digital-twin
    depends_on:
      kafka:
        condition: service_healthy
      kafka-setup:
        condition: service_completed_successfully
      postgres-digital-twin:
        condition: service_healthy
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres-digital-twin:5432/digital_twin
      - SPRING_DATASOURCE_USERNAME=digital_twin_user
      - SPRING_DATASOURCE_PASSWORD=${POSTGRES_PASSWORD:-password123}
    ports:
      - "8082:8082"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    mem_limit: 768m
    restart: unless-stopped


  # Kafka setup service to create topics after Kafka is ready
  kafka-setup:
    image: confluentinc/cp-kafka:7.6.1
    depends_on:
      kafka:
        condition: service_healthy
    command: |
      bash -c "
        echo 'Waiting for Kafka to be ready...' &&
        # Wait a bit more to ensure Kafka is fully ready
        sleep 10 &&
        # Create machine-telemetry topic for raw sensor data
        kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic machine-telemetry --partitions 6 --replication-factor 1 --config retention.ms=86400000 --config cleanup.policy=delete &&
        # Create machine-state-updates topic for state changes
        kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic machine-state-updates --partitions 3 --replication-factor 1 --config retention.ms=86400000 --config cleanup.policy=delete &&
        # Create anomaly-events topic for detected anomalies
        kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic anomaly-events --partitions 3 --replication-factor 1 --config retention.ms=86400000 --config cleanup.policy=delete &&
        # Create alerts topic for generated alerts
        kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic alerts --partitions 3 --replication-factor 1 --config retention.ms=86400000 --config cleanup.policy=delete &&
        # List created topics
        kafka-topics --bootstrap-server kafka:9092 --list &&
        echo 'Kafka topics created successfully'
      "
    volumes:
      - ./kafka-setup.log:/tmp/kafka-setup.log
    restart: "no"

  # Alert/Analytics Service
  alert-analytics:
    build:
      context: ./alert-analytics
      dockerfile: Dockerfile
    container_name: alert-analytics
    depends_on:
      kafka:
        condition: service_healthy
      kafka-setup:
        condition: service_completed_successfully
      timescaledb-alert-analytics:
        condition: service_healthy
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - SPRING_DATASOURCE_URL=jdbc:postgresql://timescaledb-alert-analytics:5432/alert_analytics
      - SPRING_DATASOURCE_USERNAME=alert_analytics_user
      - SPRING_DATASOURCE_PASSWORD=${POSTGRES_PASSWORD:-password123}
    ports:
      - "8083:8083"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    mem_limit: 600m
    restart: unless-stopped

  # Kafka UI for monitoring and management
  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    ports:
      - "9099:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: digital-twin-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_JMXPORT: 9101
    depends_on:
      kafka:
        condition: service_healthy
      kafka-setup:
        condition: service_completed_successfully
    restart: unless-stopped

volumes:
  kafka_data:
  postgres_digital_twin_data:
  timescale_alert_analytics_data: